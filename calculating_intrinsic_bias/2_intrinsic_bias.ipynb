{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "536dbd8e-68f8-4e28-9d05-2e1b0d9fbbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.1.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging->pytest) (3.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "import pytest\n",
    "import numpy as np\n",
    "from pytest import approx\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"../../../../../../../../\")\n",
    "os.chdir(\"workspace\")\n",
    "sys.path.append(\"biased-rulers/\")\n",
    "from biased_rulers.metrics import disco\n",
    "from biased_rulers.metrics import lpbs\n",
    "from biased_rulers.metrics import seat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00beb1a0-665c-4752-a369-f3224c4f6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_disco_bert_base_names(model_name):\n",
    "    os.chdir(\"biased-rulers/biased_rulers/metrics\")\n",
    "    #model = SentenceTransformer(model_name)\n",
    "    model_type = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "    print(f\"Loaded {model_type}\")\n",
    "    score = disco.disco_test(tokenizer, model)\n",
    "    os.chdir(\"../../..\")\n",
    "    # Score for the bert-base names test (taken from https://arxiv.org/pdf/2010.06032.pdf)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66f3650-8b1a-4113-905a-b8de91ad42a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lpbs_bert_base_names(model_name):\n",
    "    os.chdir(\"biased-rulers/biased_rulers/metrics\")\n",
    "    model_type = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "    print(f\"Loaded {model_type}\")\n",
    "    score = lpbs.lpbs_test(tokenizer, model)\n",
    "    os.chdir(\"../../..\")\n",
    "    # Score for the bert-base names test (taken from https://arxiv.org/pdf/2010.06032.pdf)\n",
    "    # assert score == approx(0.8, abs=1e-1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb5d06b-b164-4d8e-bd14-6fa65e117cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_seat(model_name):\n",
    "    os.chdir(\"biased-rulers/biased_rulers/metrics\")\n",
    "    model_type = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "    print(f\"Loaded {model_type}\")\n",
    "    attribute_template = \"This is the _.\"\n",
    "    target_template = \"This is the _.\"\n",
    "    results = seat.seat_test(attribute_template, target_template, tokenizer, model)\n",
    "    score = np.fromiter(results.values(), dtype=float).mean()\n",
    "    os.chdir(\"../../..\")\n",
    "    return score\n",
    "    #assert score == approx(0.4365, abs=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dbe8c5-0f24-4e46-9837-637f11ca9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_seat_tan_et_al(model_name):\n",
    "    os.chdir(\"biased-rulers/biased_rulers/metrics\")\n",
    "    model_type = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "    print(f\"Loaded {model_type}\")\n",
    "    attribute_template = \"This is the _.\"\n",
    "    target_template = \"This is the _.\"\n",
    "    results = seat.tan_et_al_test(attribute_template, target_template, tokenizer, model)\n",
    "    score = np.fromiter(results.values(), dtype=float).mean()\n",
    "    os.chdir(\"../../..\")\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157749b6-26b3-413b-9c83-88b66d2f5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_seat_lauscher_et_al(model_name):\n",
    "    os.chdir(\"biased-rulers/biased_rulers/metrics\")\n",
    "    model_type = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_type)\n",
    "    print(f\"Loaded {model_type}\")\n",
    "    attribute_template = \"This is the _.\"\n",
    "    target_template = \"This is the _.\"\n",
    "    results = seat.lauscher_et_al_test(attribute_template, target_template, tokenizer, model)\n",
    "    score = np.fromiter(results.values(), dtype=float).mean()\n",
    "    os.chdir(\"../../..\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb8dcec0-2a99-434c-abcc-4d204b53bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['google-bert/bert-base-multilingual-uncased','google-bert/bert-base-uncased', 'FacebookAI/roberta-base','FacebookAI/xlm-roberta-base','distilbert/distilbert-base-uncased', 'albert/albert-base-v2','SpanBERT/spanbert-base-cased','microsoft/deberta-base','google/electra-base-discriminator','dmis-lab/biobert-base-cased-v1.1','allenai/scibert_scivocab_uncased','bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12','microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext',\n",
    "    \"google/bert_uncased_L-2_H-128_A-2\",\n",
    "    \"google/bert_uncased_L-2_H-256_A-4\",\n",
    "    \"google/bert_uncased_L-4_H-256_A-4\",\n",
    "    \"google/bert_uncased_L-4_H-512_A-8\",\n",
    "   \"google/bert_uncased_L-6_H-512_A-8\",\n",
    "    \"google/bert_uncased_L-8_H-768_A-12\",\n",
    "    \"google/bert_uncased_L-12_H-768_A-12\"]\n",
    "def run_tests_on_models(model_names):\n",
    "    for model_name in model_names:\n",
    "        print(f\"Running tests for model: {model_name}\")\n",
    "        \n",
    "        #output1 = test_seat_lauscher_et_al(model_name)\n",
    "        #print(f\"test_seat_lauscher_et_al: {output1}\")\n",
    "        \n",
    "        #output2 = test_lpbs_bert_base_names(model_name)\n",
    "        #print(f\"test_lpbs_bert_base_names: {output2}\")\n",
    "        \n",
    "        output3 = test_disco_bert_base_names(model_name)\n",
    "        print(f\"test_disco_bert_base_names: {output3}\")\n",
    "        \n",
    "        #output4 = test_seat(model_name)\n",
    "        #print(f\"test_seat: {output4}\")\n",
    "        \n",
    "        #output5 = test_seat_tan_et_al(model_name)\n",
    "        #print(f\"test_seat_tan_et_al: {output5}\")\n",
    "        \n",
    "        #output6 = test_seat_lauscher_et_al(model_name)\n",
    "        #print(f\"test_seat_lauscher_et_al (again): {output6}\")\n",
    "        print(\"\\n\")  # Add a newline for better readability between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "295d515b-bebd-49d7-a49e-4ba90af4552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests for model: google-bert/bert-base-multilingual-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google-bert/bert-base-multilingual-uncased\n",
      "test_disco_bert_base_names: 0.592391304347826\n",
      "\n",
      "\n",
      "Running tests for model: google-bert/bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google-bert/bert-base-uncased\n",
      "test_disco_bert_base_names: 0.6777950310559007\n",
      "\n",
      "\n",
      "Running tests for model: FacebookAI/roberta-base\n",
      "Loaded FacebookAI/roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_disco_bert_base_names: 0.8051242236024845\n",
      "\n",
      "\n",
      "Running tests for model: FacebookAI/xlm-roberta-base\n",
      "Loaded FacebookAI/xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_disco_bert_base_names: 0.7841614906832298\n",
      "\n",
      "\n",
      "Running tests for model: distilbert/distilbert-base-uncased\n",
      "Loaded distilbert/distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_disco_bert_base_names: 0.6956521739130435\n",
      "\n",
      "\n",
      "Running tests for model: albert/albert-base-v2\n",
      "Loaded albert/albert-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_disco_bert_base_names: 0.672360248447205\n",
      "\n",
      "\n",
      "Running tests for model: SpanBERT/spanbert-base-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at SpanBERT/spanbert-base-cased and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SpanBERT/spanbert-base-cased\n",
      "test_disco_bert_base_names: 0.5481366459627329\n",
      "\n",
      "\n",
      "Running tests for model: microsoft/deberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForMaskedLM: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded microsoft/deberta-base\n",
      "test_disco_bert_base_names: 0.7600931677018633\n",
      "\n",
      "\n",
      "Running tests for model: google/electra-base-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraForMaskedLM: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['generator_predictions.LayerNorm.bias', 'generator_predictions.dense.weight', 'generator_lm_head.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator_lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/electra-base-discriminator\n",
      "test_disco_bert_base_names: 0.6832298136645962\n",
      "\n",
      "\n",
      "Running tests for model: dmis-lab/biobert-base-cased-v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dmis-lab/biobert-base-cased-v1.1\n",
      "test_disco_bert_base_names: 0.7888198757763976\n",
      "\n",
      "\n",
      "Running tests for model: allenai/scibert_scivocab_uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded allenai/scibert_scivocab_uncased\n",
      "test_disco_bert_base_names: 0.6358695652173914\n",
      "\n",
      "\n",
      "Running tests for model: bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_disco_bert_base_names: 0.5768633540372671\n",
      "\n",
      "\n",
      "Running tests for model: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\n",
      "test_disco_bert_base_names: 0.7934782608695652\n",
      "\n",
      "\n",
      "Running tests for model: google/bert_uncased_L-2_H-128_A-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/bert_uncased_L-2_H-128_A-2\n",
      "test_disco_bert_base_names: 0.6304347826086957\n",
      "\n",
      "\n",
      "Running tests for model: google/bert_uncased_L-2_H-256_A-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-256_A-4 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/bert_uncased_L-2_H-256_A-4\n",
      "test_disco_bert_base_names: 0.6630434782608695\n",
      "\n",
      "\n",
      "Running tests for model: google/bert_uncased_L-4_H-256_A-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/bert_uncased_L-4_H-256_A-4\n",
      "test_disco_bert_base_names: 0.7274844720496895\n",
      "\n",
      "\n",
      "Running tests for model: google/bert_uncased_L-4_H-512_A-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-512_A-8 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/bert_uncased_L-4_H-512_A-8\n",
      "test_disco_bert_base_names: 0.703416149068323\n",
      "\n",
      "\n",
      "Running tests for model: google/bert_uncased_L-6_H-512_A-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-6_H-512_A-8 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/bert_uncased_L-6_H-512_A-8\n",
      "test_disco_bert_base_names: 0.7321428571428571\n",
      "\n",
      "\n",
      "Running tests for model: google/bert_uncased_L-8_H-768_A-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-8_H-768_A-12 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/bert_uncased_L-8_H-768_A-12\n",
      "test_disco_bert_base_names: 0.7422360248447205\n",
      "\n",
      "\n",
      "Running tests for model: google/bert_uncased_L-12_H-768_A-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-12_H-768_A-12 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:6766: RuntimeWarning: divide by zero encountered in divide\n",
      "  terms = (f_obs_float - f_exp)**2 / f_exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded google/bert_uncased_L-12_H-768_A-12\n",
      "test_disco_bert_base_names: 0.5636645962732919\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_tests_on_models(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d26be-64d4-4e26-b929-32c895052438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
